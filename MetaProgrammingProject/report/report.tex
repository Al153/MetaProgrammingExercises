\documentclass{report}
\usepackage{amssymb}
\usepackage{framed}

\usepackage{amsmath}
\usepackage[a4paper,includeheadfoot,margin=2.54cm]{geometry}
\usepackage{breqn}

\usepackage{array}   % for \newcolumntype macro

\usepackage{verbatim}

\usepackage{tabstackengine}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usetikzlibrary{shadows}

\newcommand*\keystroke[1]{%
  \tikz[baseline=(key.base)]
    \node[%
      draw,
      fill=white,
      drop shadow={shadow xshift=0.25ex,shadow yshift=-0.25ex,fill=black,opacity=0.75},
      rectangle,
      rounded corners=2pt,
      inner sep=1pt,
      line width=0.5pt,
      font=\scriptsize\sffamily
    ](key) {#1\strut}
  ;
}
\setstackEOL{\cr}
\setstackgap{L}{\normalbaselineskip}




\newcommand\N[0]{\mathbb{N}}
\newcommand\Q[0]{\mathbb{Q}}

\newcommand \2[0]{\textbf{2}}
\newcommand \3[0]{\textbf{3}}


\newcommand\biimpl[0]{\Leftrightarrow}
\newcommand\impl[0]{\Rightarrow}
\newcommand\impliedBy[0]{\Leftarrow}
\newcommand \inverse[0]{^{-1}}
\newcommand \compose[0]{ \circ}

\newcommand \op[0]{^{op}}

\newcommand \cat[0]{\textbf{C}}
\newcommand \cset[0]{\textbf{Set}}

\newcommand{\todo}[1]{\textbf{#1}}

\begin{document}
\title{A Modular, Extensible, and Polymorphic Graph Query DSL}
\author{Alexander Taylor}

\maketitle
{\large
\begin{tabular}{p{5cm}p{8cm}}
Name:       & \bf Alexander Taylor      \\
CRSID:      & \bf at736 \\
College:    & \bf St John's College  \\
Module:     & \bf Metaprogramming, L305 \\
Assignment: & \bf 4 \\
Word Count: & \bf \todo{todo}\footnote{Calculated by running \texttt{texCount (http://app.uio.no/ifi/texcount/online.php)}}   \\
                   \\ 
\end{tabular}
}



\abstract
This project is an extension of my Part II project \footnote{Dissertation: https://github.com/Al153/PartIIProject/blob/master/diss/diss.pdf,\\ Code: https://github.com/Al153/PartIIProject/tree/master/src}, the purpose of which was to build a purely functional graph database system. While my Part II project focused on the full stack and optimisations of particular back-ends, this L305 project focuses on the front-end of a graph database system. The project's primary aim is to open up the interface of the DSL to more varied implementations by providing a type-level toolkit for specifying and building relational DSLs. It replicates the original DSL in a modular, tagless-final fashion, allowing implementors to choose which operations and syntax they support. Furthermore, it provides a high degree of polymorphism in the types used to represent queries, the return types of the database, and in the type-classes used by an implementation to classify which types can be stored in the database. The project also includes combinators to generate "free" implementations of parts of the specification of DSLs. Finally, I present three new back-end implementations: a simple, set based, in memory interpreter, interfacing code to plug in the back-ends for my original part II project to this part III project, and a simple optimising compiler implementation that compiles queries to a bytecode for later execution.


\tableofcontents
\newpage

\chapter{The Original Project}
\section{Introduction}
In my original part II project, I built the full stack of a graph database system. It provided a strongly typed DSL, a type-class based framework for allowing an application developer's types to be used in queries, and several different back-ends, which targeted an in-memory data-store, a PostgreSQL instance, and the LMDB memory-mapped data-store. This project acts as a DSL generator library for faster, more consistent construction` of DSLs for graph-query executors.
\subsection{The Query Language}\label{LangDef}
The original project uses an algebraic-datatype-based query language to specify queries that search for pairs of related objects and individual objects. As defined in my dissertation, the basic query constructors are as follows.

\todo{Split these up better}
\begin{equation}\label{PairQueries}
    \begin{split}
    P  &\rightarrow Rel(R) \mbox{ Find pairs related by the named relation R}\\
    &\mid RevRel(R) \mbox{ Find pairs related by the named relation R in the reverse direction}\\
    &\mid Chain(P, P) \mbox{   Find pairs related by the first sub-query followed by the second}\\
    &\mid And(P, P) \mbox{  Find pairs related by both of the sub-queries}\\
    &\mid AndRight(P, S) \mbox{  Find pairs related by P where the right value is a result of S}\\
    &\mid AndLeft(P, S) \mbox{  Find pairs related by P where the left value is a result of S}\\
    &\mid Or(P, P) \mbox{  Find pairs related by either of the sub-queries}\\
    &\mid Distinct(P) \mbox{  Find pairs related by P that are not symmetrical}\\
    &\mid Id_A \mbox{ Identity relation}\\
    &\mid Exactly(\mathit{n}, P) \mbox{  Find pairs related by n repetitions of P}\\
    &\mid Upto(\mathit{n}, P) \mbox{  Find pairs related by up to n repetitions of P}\\
    &\mid FixedPoint(P) \mbox{  Find the transitive closure of P}\\
    \end{split}
    \end{equation} 

    \begin{equation}\label{SingleQueries}
        \begin{split}
        S & \rightarrow Find(F) \mbox{ Find values that match the findable F}\\
        &\mid From(S, P) \mbox{ Find values that are reachable from results of S via P}\\
        &\mid AndS(S, S) \mbox{ Find values that are results of both sub-queries}\\
        &\mid OrS(S, S) \mbox{ Find values that are results of either sub-query}
        \end{split}
    \end{equation} 

Full details, including the type system that applies over this query language can be found in section 2.5 of my dissertation ("The Query Language", page 18).

\subsection{Executing Queries}\label{Commands}
In my Part II project, an implementation provides a \texttt{core.user.interfaces.DBExecutor} to execute constructed queries. This provides several commands for executing queries.

\begin{itemize}
    \item \texttt{find} - Finds objects matching a single query
    \item \texttt{findPairs} - Finds objects matching a pair (relational) query
    \item \texttt{shortestPath} - Finds the shortest path between two objects over the graph yielded by the query given.
    \item \texttt{allShortestPaths} - Finds the shortest path to all objects of the correct type from a source object.
    \item \texttt{insert} - inserts a set of related pairs into the database in a monadic fashion.
\end{itemize}



\section{Flaws in the Project}
There are several flaws in the front-end of this project, as necessitated by the time constraints of a part II project, as well as my desire to look at the big picture of the system rather than focusing for too long on the front-end.
\subsection{Fixed Query ADT}
The original project constructed a fixed ADT representing each query. ADT was then traversed or otherwise evaluated to interpret the results of the query. In order to get pattern-match-exhaustiveness checking, one has to seal a trait in Scala. This means that the ADT backing the DSL is fixed and cannot be directly extended or reduced in a type-safe manner. A tagless-final DSL is much easier to extend or reduce.

\subsection{Lack of Polymorphism}
The lack of polymorphism in the original DSL puts unnecessary constraints on back-end implementation.
\paragraph{Types Stored in the Database}
The original project uses the \texttt{SchemaObject} type-class to specify which types can be stored in the database. This type-class is fixed, only allowing types that can be represented as tuples of simple primitive types to be stored in the database. Allowing polymorphism over the type-class used to verify types manipulated by the DSL allows for more freedom of implementation. For example, we may wish to store more complex types to the database, such as wide floating point values, or higher order relations.

\paragraph{Fixed Return Types}
The original database interface requires implementations to return instances of the specific \texttt{Operation} monad (a stack of the future, state, error monads). And only allows read operations to return the eager \texttt{Set} type of results. That is, the return type is \texttt{Operation[Error, Set[(A, B)]]}. This specific return type constraint prevents implementors from returning more interesting values, such as compiled code for later use, or lazy alternatives to sets. Furthermore, it locked the implementation into using the same monadic, update semantics specified by the \texttt{Operation} monad.


\chapter{The New DSL}
This section introduces the new structure of the DSL.

\section{Introduction}
In the use cases of this project, I make reference to several different classes of people.

\begin{itemize}
    \item \textbf{specification writer} - The person, in this case myself, building the modules and combinators used to construct back end implementations.
    \item \textbf{Implementor} - The person using this project to build a graph query DSL. The specification of modules should be designed in such a way as to both give the implementor plenty of tools to easily produce implementations but also allow them as much flexibility as possible to do so.
    \item \textbf{Application Programmer} - The person using a graph query DSL, built by the implementor, to build an application. Ideally, Syntax provided to the application programmer should be as clean as possible with few type annotations.

\end{itemize}
 
\section{Tagless-Final}
The new DSL is a modular tagless-final based system. There are several traits to implement, each containing a subset of the operations defined is section \ref{LangDef}. This allows for implementors to only partially implement the specification or to add new modules.
The DSL is polymorphic in the type constructors \texttt{Pair} and \texttt{Single} which are used to specify queries returning pairs or single objects. This allows more flexibility than the original ADT based interface.

\section{Modularity}
The new DSL framework is built in a modular way. 
Allows more freedom to implementors to pick and choose which functionality they want to provide to the database system. This allows more specific DSLs to be designed to use particular parts of the algebra.
\subsection{Important Classes/Types/Traits}
\paragraph{Query Building}
The tagless-final interface for building queries is separated into several pieces, which can be found in package \texttt{query.dsl.components}.
\begin{itemize}
    \item \textbf{Simple Pairs} - Basic relational query operations such as \texttt{and}, \texttt{or}, \texttt{id}, etc.
    \item \textbf{Singles} - All of the single query combinators found in \ref{SingleQueries}.
    \item \textbf{Simple Repetitions} - \texttt{upto}, \texttt{exactly}
    \item \textbf{Fixed Point} - \texttt{fixedpoint} is a harder query operation to implement than \texttt{exactly} and \texttt{upto} since there is no limit on how long it will take to converge.
\end{itemize}

This allows implementors to implement the interfaces in separate parts. It also allows for construction of less expressive DSLs, for example, without the fixed point combinator.

\paragraph{Execution}
The methods from section \ref{Commands} for interpreting a query are separated out into separate interfaces for each function, straight interpretation of queries, path-finding using queries, and updates to the database. These are separated as in the case of queries to allow for partial implementation.

\paragraph{Assertion and Test Suite}
I have replicated a small subset of the unit testing suite I used in my part II project. This is implemented as a mix-in trait depending on a full DSL implementation, and a monad type-class for the return type. It consists of boolean operations for testing equality of the structure of unexecuted queries and for testing equality of the result of executing queries, an assertion operation for checking the results of tests, and a syntax module for more natural expression of tests. This suite can be found in the package \texttt{query.dsl.testing}.

\subsection{Dependency Management}\label{depManagement}
This module system allows us to delegate dependency management to the type-system. Each module is designed as a mix-in trait. It can make requirements on the rest of the implementations by requiring mix-ins using self type annotation. Hence, modules without the correct dependency on the implementation will not compile. This gives back-end implementors a set of type-level combinators for building the shape of their back-end. It also makes life easier for application writers, since the available methods are visible at compile-time, or write-time when using an IDE, such as Intellij Idea. If one was build a similar modular system in a Java style, this information would not be visible in the type signature of the database system, and could lead to exceptions being thrown at runtime due to an unimplemented method being called. The dependency management scheme can also be used by implementors to add their own extension modules. A simple example of type-level dependency management is the \texttt{query.dsl.components.BatchInserts} trait, which supplies some syntactic sugar for inserting items.

\begin{verbatim}
trait BatchInserts[M[_], ToInsert[_, _], Valid[_]] {
   self: Writes[M, ToInsert, Valid] =>
   final def inserts[A: Valid, B: Valid](xs:  ToInsert[A, B]*): M[Unit] =
        self.insert(xs.seq)
}
\end{verbatim}


The trait requires itself to be mixed in with a Writes implementation, and hence provides some syntax to the class. There are further examples of this in the sections about syntax providers and free optimisations.

\section{Syntax}
To facilitate the use of Neo-4j-like arrow syntax, there are a series of syntax provider modules. A syntax provider is a module providing the implicit conversions to allow the required syntax. Each syntax provider is a mix-in, parameter-polymorphic, trait using self-type annotations \footnote{https://docs.scala-lang.org/tour/self-types.html} to manage dependencies using the type-system as discussed in section \ref{depManagement}. Hence the application programmer need only import the DSL implementation's internal methods to get access to the syntax.

\begin{framed}
    \begin{framed}
        \begin{verbatim}
            val dsl: DSL[Type Params] = ...
            import dsl._
        
            val result = read(Knows -->--> Owns >> (Has4Wheels))
            // result: M[Se[Person, Car]]
        \end{verbatim}       
    \end{framed}
    \textit{Example of some syntax}
\end{framed}



The syntax providers are mix-in traits, rather than globally scoped objects due to the large number of polymorphic type parameters, which the Scala type-system would be unable to infer. As demonstrated below, Scala's type-system instantiates any type-parameters that it cannot immediately infer with \texttt{Nothing} types. This means that methods of providing syntax that leave un-inferred type variables require the application programmer to use excessive type annotations.
\begin{framed}
\begin{framed}
    \begin{verbatim}
        object PairSyntax {
            implicit class Syntax1[
                Pair[_, _], 
                // Scala can't infer the type arguments for Single and Valid
                Single[_], 
                Valid[_], 
                A: Valid, B: Valid
            ](pab: Pair[A, B])(
                // need to provide the DSL as an implicit parameter
                implicit pairs: PairQueries[Pair, Single, Valid]
            ) {
                def -->>(s: S[B]): Pair[A, B] = pairs.andRight(pab, s)
            }
            
            
            implicit class Syntax2[Pair, A, B](pab: Pair[A, B]) {
                // scala still cannot infer the type-class "Valid"
                def -->>[S[_], Valid[_]](s: S[B])(
                    implicit pairs: PairQueries[Pair, S, Valid],
                    va: Valid[A],
                    vb:  Valid[B]
                ): Pair[A, B] = pairs.andRight(p, s)
            }
        }
        \end{verbatim}
\end{framed}
\textit{Syntax for \texttt{AndRight}  provided by various global objects, demonstrating the need for mix in traits}
\end{framed}




The mix-in trait version does allow scala to correctly infer types.
\begin{framed}
    \begin{framed}
        \begin{verbatim}
 // type parameters are defined when  the trait is mixed in.
 trait PairSyntax[Pair[_, _], Single[_], Valid[_]] {
     // self-mix-in puts a type-level dependency check on the syntax provider.
     self: PairQueries[Pair, Single, Valid] => 
     implicit class Syntax[A: Valid, B: Valid](
         pab: Pair[A, B]
     ) {
         // no type parameters to infer
         def -->>(s: Single[B]): Pair[A, B] = self.andRight(p, s)
     }
 }
            
// Instantiate an instance using the type-level combinators
object MyDSL
    extends PairQueries[MyP, MyS, MyValid] 
    with PairSyntax[MyP, MyS, MyValid] {
        ...   
// syntax provided with an import.
import MyDSL._
val myPair: MyPair[Dog, Cat] = ...
val mySingle: MySingle[Cat] = ...
            
// syntax can be used without type annotations
val myComposite = myPair -->> mySingle
            \end{verbatim}
    \end{framed}
    \textit{Mix-in trait provides syntax without the type-inference problems encountered above}
\end{framed}



Syntax providers are kept modular in order to facilitate extended or partial implementations of the specification. 


\section{Polymorphism}
A lot of this flexibility and deferral to the scala type system is made possible by the large amount of polymorphism allowed in the system. Each trait is polymorphic in as many variables as possible in order to generalise the system to the greatest extent.

\begin{framed}
    \begin{framed}
        \begin{verbatim}
            trait DSL[
                M[_], 
                Se[_], 
                Pair[_, _], 
                Single[_], 
                Find[_], 
                Path[_], 
                ToInsert[_, _], 
                Valid[_]
            ] extends Backend[M, Se, Pair, Single, Find, Path, ToInsert, Valid]
              with PairSyntaxProvider[Pair, Single, Valid]
              with SingleSyntaxProvider[Pair, Single, Find, Valid]
              with SymmetricSyntaxProvider[Pair, Single, Valid]
              with SimplePairs[Pair, Single, Valid]
              with SimpleRepetition[Pair, Valid]
              with FixedPoint[Pair, Valid]
              with SingleQueries[Pair, Single, Find, Valid]
              with BatchInserts[M, ToInsert, Valid]
        \end{verbatim}
  
    \end{framed}
    \textit{The extremely polymorphic DSL trait is parameterised in as many types as possible.}
\end{framed}


\subsection{Query Types}
As with most tagless-final interfaces, the \texttt{Pair[A, B]} and \texttt{Single[A]} type constructors are provided as type parameters.
\subsection{Type Validation}
The \texttt{SchemaObject} type-class used in the original project to store application-programmers' types in the database is replaced by the \texttt{Valid[_]} type-class, allowing implementors to specify their own validity type-classes. This type-class specifies which types may be manipulated using the the query DSL. Implementations which store objects in a simple record-store, such as in my part II project, may use a type-class which verifies types can be converted into a record of primitive types (i.e. a tuple of types such as \texttt{Int, Float, String}), whereas authors of more complex implementations may want to be able to manipulate types such as references or higher-order relations and so use a more complex type-class to allow this.

\subsection{Return Types}
The project is also polymorphic in the return type container and sequencing type constructors. The container type, \texttt{M}, wraps the result of all operations, as it generalises the \texttt{Operation} monad used in the original project. The container type can be required to be a monad by using the \texttt{HasMonad[M]} mix-in. Other implementations may want to use another kind of type, such as \texttt{Rep} as the container type. The sequencing type constructor is the type of results returned by read queries. In the original project, it is implemented using eager \texttt{Set}s. Other implementations may want to use lazy sets instead, especially when dealing with large databases.

\chapter{Back-end implementations}

I've provided several simple implementations for this project to showcase features.
\section{Trivial implementation}
The first implementation is one that trivially uses sets  to store objects in memory. The validity type-class is the "Universe" type-class that verifies that there exists a finite universe of objects of the parameter type in memory. This implementation uses Scala's built in set methods to implement a simple specification for a database system.

\section{Original Back-End}
A second implementation is one that provides a set of connectors to allow a suitable DB instance to be used as as a L305-Project DSL instance. This is a simple case of deferring functionality to the instance's original methods and correctly filling in types.
This implementation can be found in \texttt{impl.part2} 

\begin{framed}
    \begin{framed}
        \begin{verbatim}
            ...
            = new DSL[...]
                with RuntimeTestTools[...]
                with HasMonad[Op]
                with AssertionTools[Op]
                with Lifts[...] {
                ...
                override def and[A: SchemaObject, B: SchemaObject](
                    p: FindPair[A, B], q: FindPair[A, B]
                ): FindPair[A, B] =
                    And(p, q)
                ...
                override def upto[A: SchemaObject](
                    p: FindPair[A, A], n: Int
                ): FindPair[A, A] =
                    Upto(n, p)
                ...
                override def readPair[A: SchemaObject, B: SchemaObject](
                    p: FindPair[A, B]
                ): Operation[Err, Set[(A, B)]] =
                    d.executor.findPairs(p)
                ...
                    
                override def shortestPath[A: SchemaObject](
                    start: A, end: A, p: FindPair[A, A]
                ): Operation[Err, Option[Path[A]]] =
                    d.executor.shortestPath(start, end, p)
                ...
            }
            \end{verbatim}
    \end{framed}  

    \textit{Partial example implementation of the part II back-end converter}
\end{framed}
\section{Byte Code Back-End}

In addition to these directly executing back-ends, I have also built a back-end generator for constructing back-ends that compile queries to a simple stack-based byte-code for later execution. Used this generator to create a simple bytecode based implementation which interprets the bytecode in a imperative fashion.

\subsection{Compilation Steps}
This implementation first maps tagless final terms to a type-erased AST (found in package \\\texttt{impl.bytecode.erased.adt}). That is, the object types associated with each AST term are discarded. This allows us to make optimising transformations on the AST more easily. \texttt{Upto} terms are replaced with the equivalent \texttt{Exactly} terms, as seen in section 3.11.5 of my dissertation ("Complex Subexpression Elimination", page 61. Proof can be found in appendix C). Finally, primitive relations, identity relations, and findables are converted to nodes indicating a call to external procedures provided by the sub-implementation.

The \texttt{Reads} module's methods are implemented by next compiling the AST into a bytecode program. Operations such as joins, unions, and intersections have opcode equivalents, whereas \texttt{Exactly} and \texttt{FixedPoint} operations are implemented using conditional loops. Furthermore, simple loop unrolling is used if the number of repetitions in an \texttt{Exactly} is less than 5. A binary exponentiation algorithm is used to find the fixed point.

The \texttt{Reads} module's methods finally return a \texttt{Rep} type (not the same as in LMS) whose \texttt{run} method runs the packaged interpreter over the bytecode and then uses sub-implementation-provided methods to read values of the correct type from the result.


\subsection{The Bytecode}
The bytecode consists of set of stack machine instructions and is type-parameterized by a type of \texttt{Labels} for making jumps and \texttt{Procedures} for calculating primitive relations and sets. This allows an implementation to pick types that suit the exact use case. The bytecode consists of the following instructions, and can be found in \texttt{impl.bytecode.Bytecode}

\begin{itemize}
    \item Relation manipulation: \texttt{And}, \texttt{Or}, \texttt{Join}, etc.
    \item An external procedure call: \texttt{Call}
    \item Stack manipulation instructions: \texttt{Swap}, \texttt{Dup}, \texttt{Drop}
    \item Conditional and unconditional branches \texttt{TestAndDecrement}, \texttt{TestNotEqual}, and \texttt{Jump}.
    \item Label marking (pseudo) instruction: \texttt{MarkLabel}
\end{itemize}


\subsection{Interpreters}
Sub-implementations should provide an interpreter for the bytecode. The partial implementation is parameterised in such a way that implementations may provide a safe, monadic, locked down interpreter or a faster, less safe one. Furthermore, sub-implementations should provide a \texttt{Compilable} validity type-class which allows the implementation to compile primitive relations, findables, and objects to procedure calls, and allows us to read results of the correct type from the end of executing the bytecode. This requirement on the \texttt{Compilable} type-class is expressed using a higher-order type-class. (See \texttt{impl.bytecode.BytecodeImpl} for details.) 

\subsection{Implementation}
As an example, I have constructed a simple full implementation of the bytecode interpreter back-end. It implements the bytecode interpreter in a simple, imperative, stack machine. This does not use monads to control execution or handle errors and instead simply allows any exceptions to bubble up as in a Java program. The \texttt{Compilable} type-class maps individual objects to a simple identifier which is processed by the stack machine.

\chapter{Free Implementations and Optimisation}
Since the module system exposes polymorphism over many parts of the implementation of a DSL, we can look for ways to exploit this polymorphism to reduce the burden on DSL implementors.

\section{Implementations}
Some modules of DSL implementations can be defined naturally in terms of other modules. For example, as proven in the original project in section 3.11.5 of my dissertation ("Complex Subexpression Elimination", page 61. Proof in appendix C), one can formulate the \texttt{exactly(n, P)} operation as a collection of \texttt{chain} operations. Furthermore, one can formulate \texttt{upto(n, P)} as \texttt{exactly(n, or(p, Id))}. Hence we can freely implement \texttt{exactly} and \texttt{upto} given an implementation of the \texttt{SimplePairs} module. Free implementations give us combinators for generating default methods of DSLs in a clean manner.

In general, we can construct a free implementation of a trait using mix-ins as follows:

\begin{verbatim}
trait Free[TypeParams] extends ToBeImplemented[Types] {
    self: Dependency1[Types1] with Dependency2[Types2] =>
    // implement the methods of `ToBeImplemented` using 
    // methods of `Dependency1` and `Dependency1`
    def foo[A, B](a: A): B = ...
}
\end{verbatim}

A free implementation can be used by mixing it in with implementations of the required dependencies.

\begin{verbatim}
object MyDSL extends Dependency1[Types1] with Dependency2[Types2] with Free[Types]
\end{verbatim}

As an example, I have created a free implementation of \texttt{SimpleRepetition} depending on \\ \texttt{SimplePairs} as explained above. This  implementation uses the exponentiation-by-squaring technique explained in my part II project (Section 3.11.5, "Complex Common Subexpression Elimination", page 61, proven in appendix C) and can be found in \texttt{query.dsl.free.implementation.FreeRepetitions}

One downside to overuse of free implementations is that they may not provide well optimised or performant queries for particular back ends. For example, it may be possible to carry out a back-end-specific optimisation that is not applicable in the general case.

\section{Optimisation}
There currently exist (and it may be possible to find more) examples of queries which can be optimised across many implementations. For example, \texttt{Distinct} distributes over \texttt{and} and \texttt{chain} distributes over \texttt{or}, and \texttt{and} and \texttt{or} distribute over each other. These identities can be exploited to push "narrowing" operations (which decrease the size of the result, e.g. \texttt{and}) to the leaves of the AST and "widening" operations (which increase the size of the result, e.g. \texttt{or}) to the root of the AST. This means that the majority of query evaluation handles smaller sub-queries and hence runs faster.

An additional extension, given more time, might be to construct a combinator, which given a \texttt{PairQueries} implementation, returns a new implementation that can freely apply well typed optimisations to an internal GADT before folding over the GADT to produce a query of the original type. Unfortunately, as it is cumbersome to properly fold functions over the GADT in Scala, due to the type-system, I ended up cancelling this extension to the project.


\chapter{Running The Code and Examples}
Viewing and running this project is perhaps best performed in the Intellij Idea IDE, with SBT installed, as one can inspect the inferred type of a term using \keystroke{Alt} + \keystroke{=}, follow the definition of terms using \keystroke{ctrl} + \keystroke{b}, and go back to previous views using \keystroke{ctrl} + \keystroke{alt} + \keystroke{left-arrow}/\keystroke{right-arrow}.
As the project has some dependencies on my part II project, SBT will download it from github and hence may take some time to run when the project is first compiled.

Examples can be demonstrated by running the main methods in:
\begin{itemize}
    \item \texttt{examples.TrivialExamples}
    \item \texttt{examples.BytecodeExamples}
    \item \texttt{examples.Part2Examples}
\end{itemize} 
Each class gives some example queries and also definitions of the appropriate type-class.


\chapter{Conclusion}
This project has presented a set of tools and example uses of these tools for constructing graph database domain specific languages. It has done so making heavy use of the Scala type-system to allow flexibility of implementation and to ensure type and dependency-safety.

Given more time, I would have liked to explore a construction of a type algebra to allow polymorphic optimisations to be applied, entirely provided using mix-in traits. E.g.

\begin{verbatim}
object MyFastImplementation extends FreeOptimisations(MyImplementation) 
    with DistributeJoinsUnions[...]
    with DistinctElimination[...]
    with JoinIdentity[...]
\end{verbatim}

Further, I would like to have had more time to explore bytecode compilation. I would have liked to provide a tagless-final interface for constructing bytecode programs, in order to allow the code to be compiled to more exotic back-ends, such as cross-compilation.
    
\end{document}
